{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0627c37",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T02:06:50.941234Z",
     "iopub.status.busy": "2025-10-16T02:06:50.940547Z",
     "iopub.status.idle": "2025-10-16T02:06:54.463326Z",
     "shell.execute_reply": "2025-10-16T02:06:54.462360Z"
    },
    "papermill": {
     "duration": 3.527495,
     "end_time": "2025-10-16T02:06:54.464906",
     "exception": false,
     "start_time": "2025-10-16T02:06:50.937411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "# NEW: Import the more powerful model and cross-validation tool\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training data\n",
    "X_train_full = train_df.drop('Survived', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "# Use the original test data\n",
    "X_test_full = test_df.copy() # Use copy to avoid SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0730f6da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T02:06:54.469669Z",
     "iopub.status.busy": "2025-10-16T02:06:54.469412Z",
     "iopub.status.idle": "2025-10-16T02:06:54.518148Z",
     "shell.execute_reply": "2025-10-16T02:06:54.517219Z"
    },
    "papermill": {
     "duration": 0.052713,
     "end_time": "2025-10-16T02:06:54.519701",
     "exception": false,
     "start_time": "2025-10-16T02:06:54.466988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 2. Leak-Proof Preprocessing Pipeline ---\n",
    "\n",
    "# We will apply the same steps to both train and test data,\n",
    "# but all calculations will be based ONLY on the training data.\n",
    "\n",
    "# --- Step A: Calculate statistics from training data ---\n",
    "fare_median = X_train_full['Fare'].median()\n",
    "embarked_mode = X_train_full['Embarked'].mode()[0]\n",
    "\n",
    "# --- Step B: Define a preprocessing function ---\n",
    "def preprocess(df, fare_median, embarked_mode):\n",
    "    # Make a copy to avoid changing the original data\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Fill missing values using stats from the TRAINING data\n",
    "    X['Fare'] = X['Fare'].fillna(fare_median)\n",
    "    X['Embarked'] = X['Embarked'].fillna(embarked_mode)\n",
    "\n",
    "    # Feature Engineering\n",
    "    X['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    X['Title'] = X['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "    X['Title'] = X['Title'].replace('Mlle', 'Miss')\n",
    "    X['Title'] = X['Title'].replace('Ms', 'Miss')\n",
    "    X['Title'] = X['Title'].replace('Mme', 'Mrs')\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\n",
    "    X['Title'] = X['Title'].map(title_mapping).fillna(0) # Use fillna(0) for any new titles in test data\n",
    "\n",
    "    # Impute Age based on Title medians calculated ONLY from the training data\n",
    "    # We calculate these medians inside the function but based on the training set logic\n",
    "    title_age_median = X_train_full.join(X['Title']).groupby('Title')['Age'].median()\n",
    "    X['Age'] = X.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    # If any NaNs remain (e.g., a new title in test set), fill with global training median\n",
    "    X['Age'] = X['Age'].fillna(X_train_full['Age'].median())\n",
    "\n",
    "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n",
    "    \n",
    "    # Convert categoricals\n",
    "    X['Sex'] = X['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
    "    X = pd.get_dummies(X, columns=['Embarked'], prefix='Embarked')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch']\n",
    "    X = X.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "# --- Step C: Apply the function ---\n",
    "X_train = preprocess(X_train_full, fare_median, embarked_mode)\n",
    "X_test = preprocess(X_test_full, fare_median, embarked_mode)\n",
    "\n",
    "# Align columns - crucial for ensuring test set has same columns as train set\n",
    "train_cols = X_train.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "X_test = X_test[train_cols] # Ensure order is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85da3f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T02:06:54.523786Z",
     "iopub.status.busy": "2025-10-16T02:06:54.523501Z",
     "iopub.status.idle": "2025-10-16T02:06:57.807685Z",
     "shell.execute_reply": "2025-10-16T02:06:57.806808Z"
    },
    "papermill": {
     "duration": 3.287828,
     "end_time": "2025-10-16T02:06:57.809161",
     "exception": false,
     "start_time": "2025-10-16T02:06:54.521333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HONEST cross-validation scores: [0.83240223 0.79775281 0.87078652 0.80337079 0.8258427 ]\n",
      "HONEST average cross-validation score: 0.8260\n",
      "\n",
      "Leak-proof submission file created. This score should be reliable.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model Training and Validation ---\n",
    "model = HistGradientBoostingClassifier(random_state=42, learning_rate=0.05, max_iter=200)\n",
    "\n",
    "# The new, HONEST cross-validation score\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"HONEST cross-validation scores: {cv_scores}\")\n",
    "print(f\"HONEST average cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# --- 4. Final Training and Submission ---\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': predictions.astype(int)\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nLeak-proof submission file created. This score should be reliable.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.80708,
   "end_time": "2025-10-16T02:06:58.430069",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T02:06:46.622989",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
